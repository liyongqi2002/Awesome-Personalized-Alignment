# Awesome-Personalized-Alignment

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![Stars](https://img.shields.io/github/stars/liyongqi2002/Awesome-Personalized-Alignment)](.)



## 1 Papers 


### 1.1 Position and Survey Paper

- [2024/11] **[Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)** [arXiv]
- [2024/10] **[When large language models meet personalization: perspectives of challenges and opportunities](https://doi.org/10.1007/s11280-024-01276-1)** [World Wide Web Journal]
- [2024/07] **[The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm](https://arxiv.org/abs/2406.18682)** [arXiv]
- [2024/04] **[The benefits, risks and bounds of personalizing the alignment of large language models to individuals](https://www.nature.com/articles/s42256-024-00820-y)** [Nature Machine Intelligence]
- [2024/02] **[Position: A Roadmap to Pluralistic Alignment](https://openreview.net/forum?id=gQpBnRHwxM)** [ICML 2024]
<!-- - [2024/] **[]()** [] -->


### 1.2 Personalized Preference

<!-- - [2024/] **[]()** [] -->
<!-- - [2024/] **[]()** [] -->

- [2024/10] **[Large Language Models Empowered Personalized Web Agents](https://arxiv.org/abs/2410.17236)** [arXiv]
- [2024/10] **[ComPO: Community Preferences for Language Model Personalization](https://arxiv.org/abs/2410.16027)** [arXiv]
- [2024/10] **[MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time](https://arxiv.org/abs/2410.14184)** [arXiv]
- [2024/10] **[LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education](https://arxiv.org/abs/2410.14012)** [arXiv]
- [2024/10] **[Personalized Adaptation via In-Context Preference Learning](https://arxiv.org/abs/2410.14001)** [arXiv]
- [2024/10] **[Aligning LLMs with Individual Preferences via Interaction](http://arxiv.org/abs/2410.03642)** [arXiv]
- [2024/10] **[Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](http://arxiv.org/abs/2410.08968)** [arXiv]
- [2024/10] **[PAD: Personalized Alignment at Decoding-Time](http://arxiv.org/abs/2410.04070)** [arXiv]
- [2024/10] **[MAP: Multi-Human-Value Alignment Palette](https://openreview.net/forum?id=NN6QHwgRrQ)** [OpenReview]
- [2024/10] **[PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment](https://openreview.net/forum?id=1kFDrYCuSu)** [OpenReview]
- [2024/09] **[PersonalLLM: Tailoring LLMs to Individual Preferences](http://arxiv.org/abs/2409.20296)** [arXiv]
- [2024/08] **[Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement](https://arxiv.org/abs/2402.11060)** [arXiv]
- [2024/08] **[Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning](http://arxiv.org/abs/2408.10075)** [arXiv]
- [2024/06] **[Show, Don't Tell: Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)** [arXiv]
- [2024/06] **[Few-shot Personalization of LLMs with Mis-aligned Responses](http://arxiv.org/abs/2406.18678)** [arXiv]
- [2024/06] **[Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration](https://arxiv.org/abs/2406.15951)** [EMNLP 2024]
- [2024/05] **[Aligning to Thousands of Preferences via System Message Generalization](https://arxiv.org/abs/2405.17977)** [arXiv]
- [2024/05] **[RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation](https://arxiv.org/abs/2405.00254)** [arXiv]
- [2024/04] **[The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models](https://arxiv.org/abs/2404.16019)** [arXiv]
- [2024/02] **[Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133)** [arXiv]
- [2023/10] **[Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging](https://arxiv.org/abs/2310.11564)** [arXiv]

### 1.2 Personalized Generation

- [2024/10] **[Personalized Visual Instruction Tuning](http://arxiv.org/abs/2410.07113)** [arXiv]
- [2024/06] **[Yo'LLaVA: Your Personalized Language and Vision Assistant](http://arxiv.org/abs/2406.09400)** [arXiv]
- [2024/05] **[PMG: Personalized Multimodal Generation with Large Language](https://dl.acm.org/doi/abs/10.1145/3589334.3645633)** [WWW 2024]
<!-- - [2024/] **[]()** [] -->

## 2 Workshops, Tutorials

- [2024/12] **[Pluralistic Alignment](https://pluralistic-alignment.github.io/)** [Pluralistic Alignment @ NeurIPS 2024]


<!-- ## Dataset -->

