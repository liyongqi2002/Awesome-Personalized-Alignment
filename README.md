# Awesome-Personalized-Alignment

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![Stars](https://img.shields.io/github/stars/liyongqi2002/Awesome-Personalized-Alignment)](.)



## 1 Papers 


### 1.1 Position and Survey Paper

- [2024/10] **[When large language models meet personalization: perspectives of challenges and opportunities](https://doi.org/10.1007/s11280-024-01276-1)** [World Wide Web Journal]
- [2024/07] **[The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm](https://arxiv.org/abs/2406.18682)** [arXiv]
- [2024/04] **[The benefits, risks and bounds of personalizing the alignment of large language models to individuals](https://www.nature.com/articles/s42256-024-00820-y)** [Nature Machine Intelligence]
- [2024/02] **[Position: A Roadmap to Pluralistic Alignment](https://openreview.net/forum?id=gQpBnRHwxM)** [ICML 2024]
<!-- - [2024/] **[]()** [] -->


### 1.2 Personalized Preference

<!-- - [2024/] **[]()** [] -->
<!-- - [2024/] **[]()** [] -->
- [2024/10] **[Large Language Models Empowered Personalized Web Agents](https://arxiv.org/abs/2410.17236)** [arXiv]
- [2024/10] **[ComPO: Community Preferences for Language Model Personalization](https://arxiv.org/abs/2410.16027)** [arXiv]
- [2024/10] **[MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time](https://arxiv.org/abs/2410.14184)** [arXiv]
- [2024/10] **[LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education](https://arxiv.org/abs/2410.14012)** [arXiv]
- [2024/10] **[Personalized Adaptation via In-Context Preference Learning](https://arxiv.org/abs/2410.14001)** [arXiv]
- [2024/10] **[Aligning LLMs with Individual Preferences via Interaction](http://arxiv.org/abs/2410.03642)** [arXiv]
- [2024/10] **[Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](http://arxiv.org/abs/2410.08968)** [arXiv]
- [2024/10] **[PAD: Personalized Alignment at Decoding-Time](http://arxiv.org/abs/2410.04070)** [arXiv]
- [2024/10] **[MAP: Multi-Human-Value Alignment Palette](https://openreview.net/forum?id=NN6QHwgRrQ)** [OpenReview]
- [2024/10] **[PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment](https://openreview.net/forum?id=1kFDrYCuSu)** [OpenReview]
- [2024/09] **[PersonalLLM: Tailoring LLMs to Individual Preferences](http://arxiv.org/abs/2409.20296)** [arXiv]
- [2024/08] **[Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning](http://arxiv.org/abs/2408.10075)** [arXiv]
- [2024/06] **[Few-shot Personalization of LLMs with Mis-aligned Responses](http://arxiv.org/abs/2406.18678)** [arXiv]
- [2024/06] **[Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration](https://arxiv.org/abs/2406.15951)** [EMNLP 2024]
- [2024/02] **[Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133)** [arXiv]



### 1.3 Personalized Dialogue

- [2024/10] **[Personalized Visual Instruction Tuning](http://arxiv.org/abs/2410.07113)** [arXiv]
- [2024/06] **[Yo'LLaVA: Your Personalized Language and Vision Assistant](http://arxiv.org/abs/2406.09400)** [arXiv]
<!-- - [2024/] **[]()** [] -->


<!-- ## 2 Dataset -->

