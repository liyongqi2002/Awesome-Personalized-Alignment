# Awesome-Personalized-Alignment

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![Stars](https://img.shields.io/github/stars/liyongqi2002/Awesome-Personalized-Alignment)](.)


Personalized Alignment can be broadly categorized into two directions based on usage:

1. **Personalized Assistant**: This involves developing AI assistants that are better tailored to meet users' personalized needs and preferences.

2. **Intrinsic Personalization**: This focuses on enabling AI to exhibit personalized internal characteristics and personality traits, akin to role-playing. This approach is utilized in social simulations or the development of social robots.

Our current focus is primarily on the first direction, specifically Personalized Assistants. For the latter, Intrinsic Personalization, we suggest exploring papers related to the role-playing studies of LLMs.



## 1 Papers 


### 1.1 Relevant Position and Survey Paper

- [2024/12] **[Personalized Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2412.02142)** [arXiv]
- [2024/11] **[Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)** [arXiv]
- [2024/10] **[When large language models meet personalization: perspectives of challenges and opportunities](https://doi.org/10.1007/s11280-024-01276-1)** [World Wide Web Journal]
- [2024/07] **[The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm](https://arxiv.org/abs/2406.18682)** [arXiv]
- [2024/04] **[The benefits, risks and bounds of personalizing the alignment of large language models to individuals](https://www.nature.com/articles/s42256-024-00820-y)** [Nature Machine Intelligence]
- [2024/02] **[Position: A Roadmap to Pluralistic Alignment](https://openreview.net/forum?id=gQpBnRHwxM)** [ICML 2024]
- [2024/03] **[A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications](https://arxiv.org/abs/2503.17003)** [arXiv]
<!-- - [2024/] **[]()** [] -->


### 1.2 Personalized Preference

<!-- - [2024/] **[]()** [] -->
<!-- - [2024/] **[]()** [] -->

- [2024/12] **[AI PERSONA: Towards Life-long Personalization of LLMs](https://arxiv.org/abs/2412.13103)** [arXiv]
- [2024/11] **[BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization](https://aclanthology.org/2024.findings-emnlp.398.pdf)** [EMNLP 2024]
- [2024/10] **[Large Language Models Empowered Personalized Web Agents](https://arxiv.org/abs/2410.17236)** [arXiv]
- [2024/10] **[ComPO: Community Preferences for Language Model Personalization](https://arxiv.org/abs/2410.16027)** [arXiv]
- [2024/10] **[MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time](https://arxiv.org/abs/2410.14184)** [arXiv]
- [2024/10] **[LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education](https://arxiv.org/abs/2410.14012)** [arXiv]
- [2024/10] **[Personalized Adaptation via In-Context Preference Learning](https://arxiv.org/abs/2410.14001)** [arXiv]
- [2024/10] **[Aligning LLMs with Individual Preferences via Interaction](http://arxiv.org/abs/2410.03642)** [arXiv]
- [2024/10] **[Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](http://arxiv.org/abs/2410.08968)** [arXiv]
- [2024/10] **[PAD: Personalized Alignment at Decoding-Time](http://arxiv.org/abs/2410.04070)** [arXiv]
- [2024/10] **[MAP: Multi-Human-Value Alignment Palette](https://openreview.net/forum?id=NN6QHwgRrQ)** [OpenReview]
- [2024/10] **[PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment](https://openreview.net/forum?id=1kFDrYCuSu)** [OpenReview]
- [2024/09] **[PersonalLLM: Tailoring LLMs to Individual Preferences](http://arxiv.org/abs/2409.20296)** [arXiv]
- [2024/08] **[Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement](https://arxiv.org/abs/2402.11060)** [arXiv]
- [2024/08] **[Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning](http://arxiv.org/abs/2408.10075)** [arXiv]
- [2024/06] **[Show, Don't Tell: Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)** [arXiv]
- [2024/06] **[Few-shot Personalization of LLMs with Mis-aligned Responses](http://arxiv.org/abs/2406.18678)** [arXiv]
- [2024/06] **[Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration](https://arxiv.org/abs/2406.15951)** [EMNLP 2024]
- [2024/05] **[Aligning to Thousands of Preferences via System Message Generalization](https://arxiv.org/abs/2405.17977)** [arXiv]
- [2024/05] **[RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation](https://arxiv.org/abs/2405.00254)** [arXiv]
- [2024/04] **[The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models](https://arxiv.org/abs/2404.16019)** [arXiv]
- [2024/02] **[Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133)** [arXiv]
- [2023/10] **[Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging](https://arxiv.org/abs/2310.11564)** [arXiv]
- [2024/09] **[Everyone Deserves A Reward: Learning Customized Human Preferences](https://arxiv.org/abs/2309.03126)** [arXiv]
- [2025/02] **[Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs](https://arxiv.org/abs/2309.03126)** [ICLR 2025]
- [2025/02] **[PEToolLLM: Towards Personalized Tool Learning in Large Language Models](https://arxiv.org/abs/2502.18980)** [arXiv]
- [2025/02] **[Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)** [arXiv]
- [2025/02] **[When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning](https://arxiv.org/abs/2502.19158)** [arXiv]
- [2025/03] **[Toward Multi-Session Personalized Conversation: A Large-Scale Dataset and Hierarchical Tree Framework for Implicit Reasoning](https://arxiv.org/abs/2503.07018)** [arXiv]
- [2025/03] **[From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment](https://arxiv.org/abs/2503.15463)** [arXiv]

<!-- ## Dataset -->

